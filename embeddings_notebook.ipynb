{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Amatol OCR Text Embedding Pipeline\n",
        "\n",
        "This notebook creates embeddings for all `.txt` files in the `amatol_ocr` directory structure and saves them for future use.\n",
        "\n",
        "## Overview\n",
        "- **Source**: 66+ .txt files across multiple subdirectories in `amatol_ocr/`\n",
        "- **Embedding Model**: OpenAI text-embedding-3-small\n",
        "- **Vector Store**: FAISS\n",
        "- **Features**: \n",
        "  - Recursive file discovery\n",
        "  - Document chunking\n",
        "  - Metadata preservation\n",
        "  - Embedding persistence\n",
        "  - Load existing embeddings to avoid re-computation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "from pathlib import Path\n",
        "\n",
        "# Set OpenAI API Key\n",
        "if \"OPENAI_API_KEY\" not in os.environ:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configuration loaded:\n",
            "  source_dir: amatol_ocr/amatol_sojourn\n",
            "  embeddings_dir: embeddings2\n",
            "  faiss_index_dir: embeddings2/faiss_index\n",
            "  metadata_file: embeddings2/embedding_metadata.json\n",
            "  chunk_size: 500\n",
            "  chunk_overlap: 50\n",
            "  embedding_model: text-embedding-3-small\n"
          ]
        }
      ],
      "source": [
        "import glob\n",
        "import json\n",
        "from datetime import datetime\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.docstore.document import Document\n",
        "\n",
        "# Configuration\n",
        "CONFIG = {\n",
        "    \"source_dir\": \"amatol_ocr/amatol_sojourn\",\n",
        "    \"embeddings_dir\": \"embeddings2\",\n",
        "    \"faiss_index_dir\": \"embeddings2/faiss_index\",\n",
        "    \"metadata_file\": \"embeddings2/embedding_metadata.json\",\n",
        "    \"chunk_size\": 500,\n",
        "    \"chunk_overlap\": 50,\n",
        "    \"embedding_model\": \"text-embedding-3-small\"\n",
        "}\n",
        "\n",
        "# Create embeddings directory if it doesn't exist\n",
        "os.makedirs(CONFIG[\"embeddings_dir\"], exist_ok=True)\n",
        "\n",
        "print(\"Configuration loaded:\")\n",
        "for key, value in CONFIG.items():\n",
        "    print(f\"  {key}: {value}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Discovering .txt files...\n",
            "\n",
            "Found 1 .txt files\n",
            "Total size: 0.01 MB\n",
            "Subdirectories: 0\n",
            "\n",
            "Files by subdirectory:\n",
            "  root: 1 files\n",
            "\n",
            "First 5 files:\n",
            "  1. amatol_ocr/amatol_sojourn/amatol_sojourn_article.txt\n"
          ]
        }
      ],
      "source": [
        "# File Discovery Functions\n",
        "def discover_txt_files(source_dir):\n",
        "    \"\"\"Recursively find all .txt files in the source directory.\"\"\"\n",
        "    txt_files = []\n",
        "    pattern = os.path.join(source_dir, \"**\", \"*.txt\")\n",
        "    txt_files = glob.glob(pattern, recursive=True)\n",
        "    return sorted(txt_files)\n",
        "\n",
        "def get_file_stats(file_paths):\n",
        "    \"\"\"Get statistics about the files.\"\"\"\n",
        "    stats = {\n",
        "        \"total_files\": len(file_paths),\n",
        "        \"total_size_bytes\": 0,\n",
        "        \"subdirectories\": set(),\n",
        "        \"file_types\": {}\n",
        "    }\n",
        "    \n",
        "    for file_path in file_paths:\n",
        "        # Get file size\n",
        "        stats[\"total_size_bytes\"] += os.path.getsize(file_path)\n",
        "        \n",
        "        # Get subdirectory\n",
        "        subdir = os.path.dirname(file_path).replace(CONFIG[\"source_dir\"], \"\").strip(\"/\")\n",
        "        if subdir:\n",
        "            stats[\"subdirectories\"].add(subdir)\n",
        "        \n",
        "        # Count by subdirectory\n",
        "        if subdir not in stats[\"file_types\"]:\n",
        "            stats[\"file_types\"][subdir] = 0\n",
        "        stats[\"file_types\"][subdir] += 1\n",
        "    \n",
        "    stats[\"subdirectories\"] = sorted(list(stats[\"subdirectories\"]))\n",
        "    stats[\"total_size_mb\"] = round(stats[\"total_size_bytes\"] / (1024 * 1024), 2)\n",
        "    \n",
        "    return stats\n",
        "\n",
        "# Discover files\n",
        "print(\"Discovering .txt files...\")\n",
        "txt_files = discover_txt_files(CONFIG[\"source_dir\"])\n",
        "file_stats = get_file_stats(txt_files)\n",
        "\n",
        "print(f\"\\nFound {file_stats['total_files']} .txt files\")\n",
        "print(f\"Total size: {file_stats['total_size_mb']} MB\")\n",
        "print(f\"Subdirectories: {len(file_stats['subdirectories'])}\")\n",
        "\n",
        "print(\"\\nFiles by subdirectory:\")\n",
        "for subdir, count in file_stats[\"file_types\"].items():\n",
        "    display_name = subdir if subdir else \"root\"\n",
        "    print(f\"  {display_name}: {count} files\")\n",
        "\n",
        "print(f\"\\nFirst 5 files:\")\n",
        "for i, file_path in enumerate(txt_files[:5]):\n",
        "    print(f\"  {i+1}. {file_path}\")\n",
        "    \n",
        "if len(txt_files) > 5:\n",
        "    print(f\"  ... and {len(txt_files) - 5} more\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Document Loading and Processing Functions\n",
        "def load_documents(file_paths):\n",
        "    \"\"\"Load all documents with enhanced metadata.\"\"\"\n",
        "    docs = []\n",
        "    failed_files = []\n",
        "    \n",
        "    for i, file_path in enumerate(file_paths):\n",
        "        try:\n",
        "            # Load document\n",
        "            loader = TextLoader(file_path, encoding=\"utf-8\")\n",
        "            content = loader.load()\n",
        "            \n",
        "            # Enhance metadata for each document\n",
        "            for doc in content:\n",
        "                # Extract subdirectory and filename\n",
        "                rel_path = os.path.relpath(file_path, CONFIG[\"source_dir\"])\n",
        "                subdir = os.path.dirname(rel_path)\n",
        "                filename = os.path.basename(file_path)\n",
        "                \n",
        "                # Enhanced metadata\n",
        "                doc.metadata.update({\n",
        "                    \"source_file\": filename,\n",
        "                    \"source_path\": file_path,\n",
        "                    \"relative_path\": rel_path,\n",
        "                    \"subdirectory\": subdir,\n",
        "                    \"file_size_bytes\": os.path.getsize(file_path),\n",
        "                    \"load_timestamp\": datetime.now().isoformat()\n",
        "                })\n",
        "                \n",
        "            docs.extend(content)\n",
        "            \n",
        "            # Progress update\n",
        "            if (i + 1) % 10 == 0:\n",
        "                print(f\"Loaded {i + 1}/{len(file_paths)} files...\")\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"Failed to load {file_path}: {e}\")\n",
        "            failed_files.append(file_path)\n",
        "    \n",
        "    print(f\"\\nLoading complete:\")\n",
        "    print(f\"  Successfully loaded: {len(docs)} documents\")\n",
        "    print(f\"  Failed to load: {len(failed_files)} files\")\n",
        "    \n",
        "    if failed_files:\n",
        "        print(\"Failed files:\")\n",
        "        for file_path in failed_files:\n",
        "            print(f\"  - {file_path}\")\n",
        "    \n",
        "    return docs, failed_files\n",
        "\n",
        "def chunk_documents(docs):\n",
        "    \"\"\"Split documents into chunks with metadata preservation.\"\"\"\n",
        "    print(f\"Chunking {len(docs)} documents...\")\n",
        "    \n",
        "    splitter = CharacterTextSplitter(\n",
        "        chunk_size=CONFIG[\"chunk_size\"], \n",
        "        chunk_overlap=CONFIG[\"chunk_overlap\"]\n",
        "    )\n",
        "    \n",
        "    chunks = splitter.split_documents(docs)\n",
        "    \n",
        "    # Add chunk-specific metadata\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        chunk.metadata[\"chunk_id\"] = i\n",
        "        chunk.metadata[\"chunk_size\"] = len(chunk.page_content)\n",
        "        chunk.metadata[\"chunk_timestamp\"] = datetime.now().isoformat()\n",
        "    \n",
        "    print(f\"Created {len(chunks)} chunks\")\n",
        "    \n",
        "    # Chunk statistics\n",
        "    chunk_sizes = [len(chunk.page_content) for chunk in chunks]\n",
        "    avg_chunk_size = sum(chunk_sizes) / len(chunk_sizes)\n",
        "    \n",
        "    print(f\"Average chunk size: {avg_chunk_size:.0f} characters\")\n",
        "    print(f\"Min chunk size: {min(chunk_sizes)} characters\")\n",
        "    print(f\"Max chunk size: {max(chunk_sizes)} characters\")\n",
        "    \n",
        "    return chunks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Embedding and Persistence Functions\n",
        "def check_existing_embeddings():\n",
        "    \"\"\"Check if embeddings already exist.\"\"\"\n",
        "    faiss_exists = os.path.exists(CONFIG[\"faiss_index_dir\"])\n",
        "    metadata_exists = os.path.exists(CONFIG[\"metadata_file\"])\n",
        "    \n",
        "    if faiss_exists and metadata_exists:\n",
        "        try:\n",
        "            with open(CONFIG[\"metadata_file\"], 'r') as f:\n",
        "                metadata = json.load(f)\n",
        "            print(\"Found existing embeddings:\")\n",
        "            print(f\"  Created: {metadata.get('created_at', 'Unknown')}\")\n",
        "            print(f\"  Documents: {metadata.get('num_documents', 'Unknown')}\")\n",
        "            print(f\"  Chunks: {metadata.get('num_chunks', 'Unknown')}\")\n",
        "            print(f\"  Model: {metadata.get('embedding_model', 'Unknown')}\")\n",
        "            return True, metadata\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading metadata: {e}\")\n",
        "            return False, None\n",
        "    else:\n",
        "        print(\"No existing embeddings found.\")\n",
        "        return False, None\n",
        "\n",
        "def create_embeddings(chunks):\n",
        "    \"\"\"Create embeddings and save them.\"\"\"\n",
        "    print(f\"Creating embeddings for {len(chunks)} chunks...\")\n",
        "    print(f\"Using model: {CONFIG['embedding_model']}\")\n",
        "    \n",
        "    # Initialize embeddings\n",
        "    embeddings = OpenAIEmbeddings(model=CONFIG[\"embedding_model\"])\n",
        "    \n",
        "    # Create vector store\n",
        "    print(\"Building FAISS vector store...\")\n",
        "    vectorstore = FAISS.from_documents(chunks, embeddings)\n",
        "    \n",
        "    # Save vector store\n",
        "    print(f\"Saving embeddings to {CONFIG['faiss_index_dir']}...\")\n",
        "    vectorstore.save_local(CONFIG[\"faiss_index_dir\"])\n",
        "    \n",
        "    # Create and save metadata\n",
        "    metadata = {\n",
        "        \"created_at\": datetime.now().isoformat(),\n",
        "        \"embedding_model\": CONFIG[\"embedding_model\"],\n",
        "        \"chunk_size\": CONFIG[\"chunk_size\"],\n",
        "        \"chunk_overlap\": CONFIG[\"chunk_overlap\"],\n",
        "        \"num_documents\": len(set(chunk.metadata[\"source_file\"] for chunk in chunks)),\n",
        "        \"num_chunks\": len(chunks),\n",
        "        \"source_directory\": CONFIG[\"source_dir\"],\n",
        "        \"subdirectories\": list(set(chunk.metadata[\"subdirectory\"] for chunk in chunks)),\n",
        "        \"total_characters\": sum(len(chunk.page_content) for chunk in chunks)\n",
        "    }\n",
        "    \n",
        "    with open(CONFIG[\"metadata_file\"], 'w') as f:\n",
        "        json.dump(metadata, f, indent=2)\n",
        "    \n",
        "    print(\"Embeddings created and saved successfully!\")\n",
        "    return vectorstore, metadata\n",
        "\n",
        "def load_existing_embeddings():\n",
        "    \"\"\"Load existing embeddings.\"\"\"\n",
        "    print(\"Loading existing embeddings...\")\n",
        "    embeddings = OpenAIEmbeddings(model=CONFIG[\"embedding_model\"])\n",
        "    vectorstore = FAISS.load_local(CONFIG[\"faiss_index_dir\"], embeddings)\n",
        "    \n",
        "    with open(CONFIG[\"metadata_file\"], 'r') as f:\n",
        "        metadata = json.load(f)\n",
        "    \n",
        "    print(\"Existing embeddings loaded successfully!\")\n",
        "    return vectorstore, metadata\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Created a chunk of size 614, which is longer than the specified 500\n",
            "Created a chunk of size 773, which is longer than the specified 500\n",
            "Created a chunk of size 920, which is longer than the specified 500\n",
            "Created a chunk of size 559, which is longer than the specified 500\n",
            "Created a chunk of size 590, which is longer than the specified 500\n",
            "Created a chunk of size 544, which is longer than the specified 500\n",
            "Created a chunk of size 896, which is longer than the specified 500\n",
            "Created a chunk of size 1256, which is longer than the specified 500\n",
            "Created a chunk of size 510, which is longer than the specified 500\n",
            "Created a chunk of size 642, which is longer than the specified 500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "AMATOL OCR EMBEDDING PIPELINE\n",
            "============================================================\n",
            "No existing embeddings found.\n",
            "\n",
            "No existing embeddings found. Creating new embeddings...\n",
            "\n",
            "Loading complete:\n",
            "  Successfully loaded: 1 documents\n",
            "  Failed to load: 0 files\n",
            "Chunking 1 documents...\n",
            "Created 19 chunks\n",
            "Average chunk size: 492 characters\n",
            "Min chunk size: 7 characters\n",
            "Max chunk size: 1256 characters\n",
            "Creating embeddings for 19 chunks...\n",
            "Using model: text-embedding-3-small\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_25989/3993471105.py:30: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
            "  embeddings = OpenAIEmbeddings(model=CONFIG[\"embedding_model\"])\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Building FAISS vector store...\n",
            "Saving embeddings to embeddings2/faiss_index...\n",
            "Embeddings created and saved successfully!\n",
            "\n",
            "============================================================\n",
            "EMBEDDINGS CREATED SUCCESSFULLY\n",
            "============================================================\n",
            "\n",
            "Final Statistics:\n",
            "  Documents processed: 1\n",
            "  Total chunks: 19\n",
            "  Total characters: 9,346\n",
            "  Average characters per chunk: 491\n",
            "  Subdirectories: 1\n",
            "  Embedding model: text-embedding-3-small\n",
            "  Storage location: embeddings2/faiss_index\n",
            "\n",
            "‚úÖ Embeddings are ready for use!\n"
          ]
        }
      ],
      "source": [
        "# Main Execution Pipeline\n",
        "print(\"=\" * 60)\n",
        "print(\"AMATOL OCR EMBEDDING PIPELINE\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Check for existing embeddings\n",
        "has_existing, existing_metadata = check_existing_embeddings()\n",
        "\n",
        "if has_existing:\n",
        "    choice = input(\"\\nExisting embeddings found. Do you want to:\\n1. Load existing embeddings\\n2. Recreate embeddings (will overwrite)\\nEnter choice (1 or 2): \").strip()\n",
        "    \n",
        "    if choice == \"1\":\n",
        "        vectorstore, metadata = load_existing_embeddings()\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(\"EMBEDDINGS LOADED SUCCESSFULLY\")\n",
        "        print(\"=\" * 60)\n",
        "        print(f\"Total chunks available: {metadata['num_chunks']}\")\n",
        "        print(f\"Total documents: {metadata['num_documents']}\")\n",
        "    else:\n",
        "        print(\"\\nProceeding with recreation...\")\n",
        "        # Load and process documents\n",
        "        docs, failed_files = load_documents(txt_files)\n",
        "        chunks = chunk_documents(docs)\n",
        "        vectorstore, metadata = create_embeddings(chunks)\n",
        "        \n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(\"EMBEDDINGS CREATED SUCCESSFULLY\")\n",
        "        print(\"=\" * 60)\n",
        "else:\n",
        "    print(\"\\nNo existing embeddings found. Creating new embeddings...\")\n",
        "    # Load and process documents\n",
        "    docs, failed_files = load_documents(txt_files)\n",
        "    chunks = chunk_documents(docs)\n",
        "    vectorstore, metadata = create_embeddings(chunks)\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"EMBEDDINGS CREATED SUCCESSFULLY\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "# Display final statistics\n",
        "print(f\"\\nFinal Statistics:\")\n",
        "print(f\"  Documents processed: {metadata['num_documents']}\")\n",
        "print(f\"  Total chunks: {metadata['num_chunks']}\")\n",
        "print(f\"  Total characters: {metadata['total_characters']:,}\")\n",
        "print(f\"  Average characters per chunk: {metadata['total_characters'] // metadata['num_chunks']:,}\")\n",
        "print(f\"  Subdirectories: {len(metadata['subdirectories'])}\")\n",
        "print(f\"  Embedding model: {metadata['embedding_model']}\")\n",
        "print(f\"  Storage location: {CONFIG['faiss_index_dir']}\")\n",
        "\n",
        "print(f\"\\n‚úÖ Embeddings are ready for use!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üîç Testing similarity search with query: 'Amatol plant production'\n",
            "--------------------------------------------------\n",
            "Found 3 relevant chunks:\n",
            "\n",
            "Result 1:\n",
            "  Source: amatol_sojourn_article.txt\n",
            "  Subdirectory: \n",
            "  Chunk ID: 10\n",
            "  Content preview: The town reached a peak population of 7,000, had the capacity to house over 10,000, and was planned to accommodate a possible population of 25,000. \n",
            "\n",
            "Amatol Plant...\n",
            "\n",
            "Result 2:\n",
            "  Source: amatol_sojourn_article.txt\n",
            "  Subdirectory: \n",
            "  Chunk ID: 11\n",
            "  Content preview: Loading operations at the Amatol plant began on July 31, 1918, and on August 3, the first shell was loaded. The plant was capable of loading ‚Äú60,000 shells of all sizes, 50,000 boosters, 50,000 hand g...\n",
            "\n",
            "Result 3:\n",
            "  Source: amatol_sojourn_article.txt\n",
            "  Subdirectory: \n",
            "  Chunk ID: 8\n",
            "  Content preview: Development was rapid. By June, the population was nearly 2,000, and train service was added. By August, advertisements for Amatol described it as ‚Äúa city with all modern improvements with electric li...\n",
            "\n",
            "\n",
            "üîç Testing similarity search with query: 'explosion accident'\n",
            "--------------------------------------------------\n",
            "Found 3 relevant chunks:\n",
            "\n",
            "Result 1:\n",
            "  Source: amatol_sojourn_article.txt\n",
            "  Subdirectory: \n",
            "  Chunk ID: 14\n",
            "  Content preview: Conclusion\n",
            "\n",
            "In just a matter of months after signing the Armistice, the population of Amatol had virtually vanished, with only a few hundred remaining there. The former munitions complex experienced a...\n",
            "\n",
            "Result 2:\n",
            "  Source: amatol_sojourn_article.txt\n",
            "  Subdirectory: \n",
            "  Chunk ID: 6\n",
            "  Content preview: After the final determination of the site had been made, the Atlantic Loading Company immediately went to work, which caught neighboring towns by surprise. The Tuckerton Beacon reported that on March ...\n",
            "\n",
            "Result 3:\n",
            "  Source: amatol_sojourn_article.txt\n",
            "  Subdirectory: \n",
            "  Chunk ID: 11\n",
            "  Content preview: Loading operations at the Amatol plant began on July 31, 1918, and on August 3, the first shell was loaded. The plant was capable of loading ‚Äú60,000 shells of all sizes, 50,000 boosters, 50,000 hand g...\n",
            "\n",
            "\n",
            "üîç Testing similarity search with query: 'newspaper article'\n",
            "--------------------------------------------------\n",
            "Found 3 relevant chunks:\n",
            "\n",
            "Result 1:\n",
            "  Source: amatol_sojourn_article.txt\n",
            "  Subdirectory: \n",
            "  Chunk ID: 13\n",
            "  Content preview: There were also Camp Amatol sports teams. Basketball was a big sport played amongst soldiers, including officers. A December 6, 1918 article records, ‚Äúthe Hammonton five defeated the U.S. Ordnance qui...\n",
            "\n",
            "Result 2:\n",
            "  Source: amatol_sojourn_article.txt\n",
            "  Subdirectory: \n",
            "  Chunk ID: 18\n",
            "  Content preview: As the centennial of America‚Äôs participation in the First World War draws to a close, it is good to take a moment to remember a little town, built in a forest in New Jersey, which helped with the war ...\n",
            "\n",
            "Result 3:\n",
            "  Source: amatol_sojourn_article.txt\n",
            "  Subdirectory: \n",
            "  Chunk ID: 2\n",
            "  Content preview: History...\n",
            "\n",
            "üß™ Test functions ready. Uncomment the test queries above to run similarity searches.\n"
          ]
        }
      ],
      "source": [
        "# Test the Embeddings with Similarity Search\n",
        "def test_similarity_search(vectorstore, query, k=5):\n",
        "    \"\"\"Test the vector store with a sample query.\"\"\"\n",
        "    print(f\"\\nüîç Testing similarity search with query: '{query}'\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    # Perform similarity search\n",
        "    docs = vectorstore.similarity_search(query, k=k)\n",
        "    \n",
        "    print(f\"Found {len(docs)} relevant chunks:\")\n",
        "    print()\n",
        "    \n",
        "    for i, doc in enumerate(docs, 1):\n",
        "        print(f\"Result {i}:\")\n",
        "        print(f\"  Source: {doc.metadata.get('source_file', 'Unknown')}\")\n",
        "        print(f\"  Subdirectory: {doc.metadata.get('subdirectory', 'Unknown')}\")\n",
        "        print(f\"  Chunk ID: {doc.metadata.get('chunk_id', 'Unknown')}\")\n",
        "        print(f\"  Content preview: {doc.page_content[:200]}...\")\n",
        "        print()\n",
        "    \n",
        "    return docs\n",
        "\n",
        "# Example test queries - uncomment to run\n",
        "test_similarity_search(vectorstore, \"Amatol plant production\", k=3)\n",
        "test_similarity_search(vectorstore, \"explosion accident\", k=3) \n",
        "test_similarity_search(vectorstore, \"newspaper article\", k=3)\n",
        "\n",
        "print(\"üß™ Test functions ready. Uncomment the test queries above to run similarity searches.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üõ†Ô∏è  UTILITY FUNCTIONS LOADED\n",
            "Available functions:\n",
            "  - get_embedding_info(): Display embedding statistics\n",
            "  - delete_embeddings(): Delete all embeddings (with confirmation)\n",
            "  - search_by_source(vectorstore, 'filename.txt'): Find chunks from specific file\n",
            "  - test_similarity_search(vectorstore, 'query'): Test semantic search\n"
          ]
        }
      ],
      "source": [
        "# Utility Functions for Working with Embeddings\n",
        "\n",
        "def get_embedding_info():\n",
        "    \"\"\"Display detailed information about the current embeddings.\"\"\"\n",
        "    if os.path.exists(CONFIG[\"metadata_file\"]):\n",
        "        with open(CONFIG[\"metadata_file\"], 'r') as f:\n",
        "            metadata = json.load(f)\n",
        "        \n",
        "        print(\"üìä EMBEDDING INFORMATION\")\n",
        "        print(\"=\" * 50)\n",
        "        print(f\"Created: {metadata['created_at']}\")\n",
        "        print(f\"Model: {metadata['embedding_model']}\")\n",
        "        print(f\"Source directory: {metadata['source_directory']}\")\n",
        "        print(f\"Documents: {metadata['num_documents']}\")\n",
        "        print(f\"Chunks: {metadata['num_chunks']}\")\n",
        "        print(f\"Total characters: {metadata['total_characters']:,}\")\n",
        "        print(f\"Chunk size: {metadata['chunk_size']}\")\n",
        "        print(f\"Chunk overlap: {metadata['chunk_overlap']}\")\n",
        "        print(f\"Subdirectories: {', '.join(metadata['subdirectories'])}\")\n",
        "        \n",
        "        return metadata\n",
        "    else:\n",
        "        print(\"‚ùå No embedding metadata found.\")\n",
        "        return None\n",
        "\n",
        "def delete_embeddings():\n",
        "    \"\"\"Delete existing embeddings and metadata.\"\"\"\n",
        "    import shutil\n",
        "    \n",
        "    confirm = input(\"‚ö†Ô∏è  Are you sure you want to delete all embeddings? (yes/no): \").strip().lower()\n",
        "    \n",
        "    if confirm == \"yes\":\n",
        "        if os.path.exists(CONFIG[\"faiss_index_dir\"]):\n",
        "            shutil.rmtree(CONFIG[\"faiss_index_dir\"])\n",
        "            print(f\"üóëÔ∏è  Deleted {CONFIG['faiss_index_dir']}\")\n",
        "        \n",
        "        if os.path.exists(CONFIG[\"metadata_file\"]):\n",
        "            os.remove(CONFIG[\"metadata_file\"])\n",
        "            print(f\"üóëÔ∏è  Deleted {CONFIG['metadata_file']}\")\n",
        "        \n",
        "        print(\"‚úÖ All embeddings deleted.\")\n",
        "    else:\n",
        "        print(\"‚ùå Deletion cancelled.\")\n",
        "\n",
        "def search_by_source(vectorstore, source_filename, k=10):\n",
        "    \"\"\"Find all chunks from a specific source file.\"\"\"\n",
        "    print(f\"üîç Searching for chunks from: {source_filename}\")\n",
        "    \n",
        "    # Get all documents and filter by source\n",
        "    # Note: This is a simple approach. For large datasets, you might want to use metadata filtering\n",
        "    all_docs = vectorstore.similarity_search(\"\", k=1000)  # Get many docs\n",
        "    \n",
        "    matching_docs = [doc for doc in all_docs if doc.metadata.get('source_file') == source_filename]\n",
        "    \n",
        "    print(f\"Found {len(matching_docs)} chunks from {source_filename}\")\n",
        "    \n",
        "    for i, doc in enumerate(matching_docs[:k], 1):\n",
        "        print(f\"\\nChunk {i}:\")\n",
        "        print(f\"  Chunk ID: {doc.metadata.get('chunk_id', 'Unknown')}\")\n",
        "        print(f\"  Size: {doc.metadata.get('chunk_size', 'Unknown')} chars\")\n",
        "        print(f\"  Preview: {doc.page_content[:150]}...\")\n",
        "    \n",
        "    return matching_docs[:k]\n",
        "\n",
        "# Display utility info\n",
        "print(\"üõ†Ô∏è  UTILITY FUNCTIONS LOADED\")\n",
        "print(\"Available functions:\")\n",
        "print(\"  - get_embedding_info(): Display embedding statistics\")\n",
        "print(\"  - delete_embeddings(): Delete all embeddings (with confirmation)\")\n",
        "print(\"  - search_by_source(vectorstore, 'filename.txt'): Find chunks from specific file\")\n",
        "print(\"  - test_similarity_search(vectorstore, 'query'): Test semantic search\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Summary & Next Steps\n",
        "\n",
        "This notebook provides a complete pipeline for embedding the Amatol OCR text files:\n",
        "\n",
        "### ‚úÖ What This Notebook Does:\n",
        "1. **Discovers** all .txt files recursively in the `amatol_ocr/` directory\n",
        "2. **Loads** documents with enhanced metadata (source, subdirectory, timestamps)\n",
        "3. **Chunks** documents into manageable pieces for embedding\n",
        "4. **Creates** embeddings using OpenAI's text-embedding-3-small model\n",
        "5. **Saves** embeddings and metadata to avoid re-computation\n",
        "6. **Provides** utilities for testing and working with the embeddings\n",
        "\n",
        "### üîß Key Features:\n",
        "- **Smart caching**: Checks for existing embeddings and offers to load them\n",
        "- **Rich metadata**: Tracks source files, subdirectories, and processing details\n",
        "- **Progress tracking**: Shows loading and chunking progress\n",
        "- **Error handling**: Reports files that failed to load\n",
        "- **Testing tools**: Built-in similarity search functions\n",
        "\n",
        "### üìÅ Output Structure:\n",
        "```\n",
        "embeddings/\n",
        "‚îú‚îÄ‚îÄ faiss_index/           # FAISS vector store files\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ index.faiss\n",
        "‚îÇ   ‚îî‚îÄ‚îÄ index.pkl\n",
        "‚îî‚îÄ‚îÄ embedding_metadata.json  # Processing metadata\n",
        "```\n",
        "\n",
        "### üöÄ Next Steps:\n",
        "1. Run all cells to create your embeddings\n",
        "2. Test with similarity searches using the provided functions\n",
        "3. Use the `vectorstore` object for your RAG applications\n",
        "4. Run `get_embedding_info()` to view embedding statistics anytime\n",
        "\n",
        "### üí° Usage Examples:\n",
        "```python\n",
        "# Test semantic search\n",
        "test_similarity_search(vectorstore, \"your query here\", k=5)\n",
        "\n",
        "# Find content from specific file\n",
        "search_by_source(vectorstore, \"filename.txt\")\n",
        "\n",
        "# Get embedding statistics\n",
        "get_embedding_info()\n",
        "```\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
